{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/ubuntu/FantasyFootballInjuryIQ/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 73\u001b[0m\n\u001b[1;32m     63\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayer had a knee injury last season.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     ]\n\u001b[1;32m     70\u001b[0m }\n\u001b[1;32m     72\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m---> 73\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_extraction\u001b[39m(df):\n\u001b[0;32m---> 39\u001b[0m     sentiment_analyzer, ner_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Apply preprocessing and stemming\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(preprocess_and_stem)\n",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_models\u001b[39m():\n\u001b[1;32m      2\u001b[0m     sentiment_analyzer \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m----> 3\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_model\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     ner_model \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         model\u001b[38;5;241m=\u001b[39mAutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m         aggregation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_analyzer, ner_model\n",
      "File \u001b[0;32m~/FantasyFootballInjuryIQ/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1637\u001b[0m, in \u001b[0;36m__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "File \u001b[0;32m~/FantasyFootballInjuryIQ/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1616\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "def load_models():\n",
    "    sentiment_analyzer = pipeline(\n",
    "        model=AutoModelForSequenceClassification.from_pretrained(\"sentiment_model\"),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"sentiment_model\")\n",
    "    )\n",
    "    ner_model = pipeline(\n",
    "        \"ner\",\n",
    "        model=AutoModelForTokenClassification.from_pretrained(\"ner_model\"),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"ner_model\"),\n",
    "        aggregation_strategy=\"simple\"\n",
    "    )\n",
    "    return sentiment_analyzer, ner_model\n",
    "def preprocess_and_stem(text):\n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def feature_extraction(df):\n",
    "    sentiment_analyzer, ner_model = load_models()\n",
    "\n",
    "    # Apply preprocessing and stemming\n",
    "    df[\"Processed\"] = df[\"Combined\"].progress_apply(preprocess_and_stem)\n",
    "\n",
    "    # Apply sentiment analysis with progress bar\n",
    "    df[\"sentiment\"] = df[\"Processed\"].progress_apply(lambda x: sentiment_analyzer(x)[0]['label'])\n",
    "\n",
    "    # Apply NER with progress bar\n",
    "    df[\"ner_entities\"] = df[\"Processed\"].progress_apply(ner_model)\n",
    "\n",
    "    sentiment_map = {\"POSITIVE\": 1, \"NEGATIVE\": 0}\n",
    "    df['sentiment_numeric'] = df['sentiment'].map(sentiment_map)\n",
    "\n",
    "    injury_terms = [\"ankle\", \"knee\", \"hamstring\", \"groin\", \"thigh\", \"head\", \"concussion\", \"neck\", \"shoulder\"]\n",
    "    for term in injury_terms:\n",
    "        df[f'{term}_injury'] = df['ner_entities'].progress_apply(\n",
    "            lambda entities: any(term in e['word'] for e in entities)\n",
    "        )\n",
    "\n",
    "    return df[[\"Combined\", \"Processed\", \"sentiment\", \"sentiment_numeric\", \"ner_entities\", \"ankle_injury\", \"knee_injury\", \"hamstring_injury\", \"groin_injury\", \"thigh_injury\", \"head_injury\", \n",
    "               \"concussion_injury\", \"neck_injury\", \"shoulder_injury\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('playersWithInjurie10-2-24.pickle', 'rb') as handle:\n",
    "    all_injuries_df = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727906993.535082    5394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 22:09:53.579811: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "100%|██████████| 27661/27661 [57:14<00:00,  8.05it/s] \n",
      "100%|██████████| 27661/27661 [5:04:10<00:00,  1.52it/s]  \n",
      "100%|██████████| 27661/27661 [00:00<00:00, 524890.48it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 528708.08it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 517718.49it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 519899.28it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 517482.95it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 523592.36it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 521788.56it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 536194.94it/s]\n",
      "100%|██████████| 27661/27661 [00:00<00:00, 531014.22it/s]\n"
     ]
    }
   ],
   "source": [
    "temp = feature_extraction(all_injuries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp10-3-24-2.pickle', 'wb') as handle:\n",
    "    pickle.dump(temp,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def save_models():\n",
    "    # Save sentiment analysis model\n",
    "    sentiment_analyzer = pipeline(\n",
    "        model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "    )\n",
    "    sentiment_analyzer.model.save_pretrained(\"sentiment_model2\")\n",
    "    sentiment_analyzer.tokenizer.save_pretrained(\"sentiment_model2\")\n",
    "\n",
    "    # Save NER model\n",
    "    ner_model = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    ner_model.model.save_pretrained(\"ner_model\")\n",
    "    ner_model.tokenizer.save_pretrained(\"ner_model\")\n",
    "\n",
    "# Call save_models once to save the models\n",
    "save_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
